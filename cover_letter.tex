\documentclass[11pt]{article}
\usepackage[inner=2cm,outer=2cm, top=2cm, bottom=2cm]{geometry}               % See geometry.pdf to learn the layout options. There are lots.

\usepackage{pseudocode}
\usepackage{url}

\newcommand{\stmt}[1]{{\footnotesize $\langle$\stmttt#1\relax$\rangle$}}

\title{Responses to reviewers' comments and list of modifications done\\ on paper SORO226\\
\LARGE{\textbf{Grounding the interaction: anchoring situated discourse in everyday human-robot interaction}}}
\author{S\'everin Lemaignan \and Raquel Ros \and E. Akin Sisbot \and Rachid Alami \and Michael Beetz}


\begin{document}

\maketitle

The authors would like to thank the reviewers for their constructive feedback
that helped us to strengthen the paper.

We next answer the reviewers comments and suggestions. Besides, each answer
indicates the changes made in the paper to address the reviewers' concerns. An
important number of language corrections have been made, and the paper has been
proof-read by a native English speaker.

%We are very grateful to the reviewers for having accepted to review our 
%work, for the care they took in examining it and their resulting 
%comments which have been appreciated in order to improve the quality of
%the paper.

%Several reviewers noted linguistic issues and terminology/typography 
%inconsistencies. We have done our best to carefully proof-read the article,
%with the help of a native English speaker.

%In the following three sections, we address separately each of
%the concerns of our three reviewers.

\section{Reviewer 1}
%\section{Comments of the first reviewer}

\subsection{How robust is the approach with regard to perceptual recognition
errors?}

We are aware that perception is not robust and therefore one of our next steps
is to include a probabilistic layer to the approach (as already mentioned in
the discussion section). We believe though that a robot (specially interacting
with people) should be allowed to make mistakes due to perception noise, or not
to be sure about something (for instance, ``I think that..."), which represents
the uncertainty the robot has about a fact. In the meantime, to overcome
perception errors, the robot provides visual feedback (looking at objects,
showing in screen its 3D model of the environment) to the user when possible
and/or verifies the information with the user whenever is needed (for instance,
``do you mean the box\_01"?). 

\subsection{Does the cognitive model enable a meaningful interaction with a
real user in a real-world setting?}

Due to perception problems and computational delays, fully realistic settings
are not yet possible. In static settings, where neither humans nor objects
move, the interaction with real users is meaningful and the flow of information
between robot and user is accurate enough.  We have developed some experiments
where the user interacts with the system only through a keyboard (no gestures
are included) asking questions, providing information, etc. and the outcome is
quite promising. However, for dynamic settings, where the user can point at
things, move the objects around, etc., the interaction is not fast enough and
the user should perform slow movements for the robot to create a consistent
state of world.

A recent video showing the current state of the interaction can be viewed
here: \url{http://www.youtube.com/watch?v=IODx50uV_k4}

\subsection{Rework of the introduction} 

%Reviewer comment: ``The paper starts off with a discussion
%of Austins speech act theory, including locutionary act, illocutionary force
%and perlocutionary act. As the concept of speech acts is not really picked up
%in the remainder of the paper, I do not see the point in starting with such a
%discussion. It would be more appropriate to motivate how a robot could benefit
%from a symbolic knowledge base etc.''

The analysis of dialogue understanding we have followed has been inspired by
Austin's \emph{speech acts}. Thus, we believe it is worthwhile to briefly mention
them in the introduction as a theoretical ground of our work. 

However, since we do not directly refer to them in the remainder of the paper,
we refer to Austin's definitions in a footnote.

\section{Reviewer 2}
% \section{Comments of the second reviewer}

\begin{itemize}
\item All reviewer's comments regarding citations have been addressed.
\item The reviewer's remark on human tendency to explain in a consistent way the world has been added.
\item Language issues have been addressed as well.
\end{itemize}

\section{Reviewer 3}
%\section{Comments of the third reviewer}

\subsection{``Explain the relation of the presented approach to the Open World
Assumption''}

The Open World assumption mainly allows us to model that we may not know if a
fact is true or not. A common example is the {\tt knowsAbout} predicate. This
predicate defines if some agent knows about some object. It is important
(especially for dialog) to model that an agent knows or  does not know about an
object, or that the robot simply does not know if the human knows about it.

The Open World Assumption has important consequences on the reasoning
abilities of the system, since it prevents the classical ``negation by 
failure" and leads to tighter modelling requirement to locally ``close the
world" (for instance, the {\tt Color} class is defined as the equivalent of
the union of the 10 or so color instances we consider).

\subsection{``Present a systematic overview of the formalisms and
representations used (OWL, RDF, DL, ...), and choose one consistent syntax for
listing the examples''}

Paragraph 2.2 has been reorganized to clarify the vocabulary and better
define the various acronyms.

\subsection{``The authors are encouraged to explain the notion of curiosity
with respect to the Dora robot by Hawes et al. (2011, ICRA) and Hanheide et al.
(2011, IJCAI)''}

A reference to the Dora robot has been added in section 2.1. However, at the
time when this work had been submitted for review, neither of the two proposed
papers by the reviewer were available (and in fact, IJCAI is taking place this
week).

\subsection{``Give examples of the alignment of ORO with OpenCyc''}

Numerous predicates in {\tt commonsense.oro.owl} are defined in the OpenCyc
namespace: {\tt farFrom, near, hold-Underspecified, objectFoundInLocation} are
some of examples. Similarly, most of the upper classes in our ontology belong
to OpenCyc (e.g. {\tt SpatialThing, PartiallyTangible, TemporalThing},...).
Overall, about 50 entities (from the more than 180 defined in {\tt
commonsense.oro.owl}) come from OpenCyc.

Since most of the upper classes are OpenCyc concepts, this should 
guarantee, to a certain extend, the alignment of our ontology with other ontologies.

\subsection{Typography and notation consistency}

Typography has been throughly revised for better consistency. Notation variants have been
made explicit (for instance, section 2.4).

\subsection{``p.6, II.E. Explain ``multi-lingual support"''}

The ``multi-lingual support" part has been reworded (Section 2.5.1).

\subsection{``Points At: please describe how you manage the critical aspect of
timing''}

SPARK is continuously updating the human non-verbal queues included in this
work (i.e., looking at, pointing at, etc.). The outcome of these computation
(facts like {\tt $\langle$human1 pointsAt bottle\_01$\rangle$}) are then
constantly been sent to the knowledge base.

In the current approach, the timing aspect is managed during the analysis of
dialogue. When the user makes reference to an object through demonstratives
(e.g. this, that), the DIALOGS module queries the ontology for objects being
pointed at. The assumption for this mechanism to work is that the human is
constantly pointing while the dialog analysis takes place.  Both situation
assessment and dialogue processing chains induce small delays, but these are
roughly the same (below 200ms), and compensate for each other. 

Section 3.1. and 5.3.3. clarify this point. 

\subsection{``p.8, III.B.Location according to an object: the authors are
kindly referred to the work done by, e.g., K. Sj\"o\"o or J. Kelleher''}

Kelleher's work, as well as Blisard's, are now cited in the section.

\subsection{``p.8, III.B. "creation of imaginary objects...hinted by verbal
assertions..." -- this phenomenon is called presupposition accommodation''}

We thank the reviewer for this input, which has been added to the article.

\subsection{``p.9, Alg. IV.1: the crucial bits are left out; please explain
``GenerateDescription", ``Ontology.Find", ``Ontology.CheckEquivalent", and
``Discrimination"''}

{\tt GenerateDescription} translates the parsing tree of each nominal group
into statements. A set of rules (like the one for adjectives mentioned 
in footnote, section 5.1) help with this task.

The detailled algorithm is provided in appendix to this letter.  We would be
happy to add it to the paper as well if the editor wants to consider it.

{\tt Ontlogy.Find} and {\tt Ontlogy.CheckEquivalent} are simple, atomic calls to
the ontology server. The first one send a SPARQL query build from the set of 
partial statement build at the {\tt GenerateDescription} step, the second
one check for each candidate pair if {\tt candidate1 owl:sameAs candidate2}.
This check uses the classified knowledge model.

The {\tt Discrimation} algorithm is explained in details in Ros2010. The
reference has been made more explicit.

\subsection{``how does the system deal with restrictive information vs.
attributive information expressed by adjectives (e.g., in ``the yellow banana is
big")?''}

When sentences like ``the yellow banana is big" are processed, they are
recognized as statements to be added to the ontology because of the state verb ``to be''
({\sc Dialogs} maintains a list of such verbs).

The nominal group is first resolved as usual (``the yellow banana''), and a new statement
is then generated that describe the new feature to be added to the concept.

Please refer to the {\sc GenerateDescription} algorithm in appendix to see how the
new statements are created.

\subsection{``Mention work on Generation and Resolution of Referring Expressions''}

Thanks for pointing out the work that has been done in this research community.
Even if our contribution focuses more on the grounding of concepts from 
multi-modal knowledge source, these contribution are of definitive 
interest for us.

A reference to Zender's work has been added to the \emph{Related work} 
section, along with a reference to Dale and Reiter paper from 1995.

\subsection{``Present an initial overview of the
robot hardware and the common software architecture used in the systems''}

Paragraph added in the section 6 introduction.

\subsection{``VI. which university in Munich? LMU or TUM?''}

The study was conducted with TUM in Munich. This has been corrected in 
the paper as well.

\subsection{``VI. please present the anecdote of unexpected behavior elsewhere
or give some more context''}


\section{Appendix}

\subsection{GenerateDescription algorithms}
\label{GenerateDescription}
\begin{pseudocode}[ruled]{GenerateDescription}{group}
\label{algo|GenerateDescription}

\PROCEDURE{GenerateDescription}{group} 
   noun \GETS \CALL{GetNoun}{group} \\ 
   \IF \CALL{Ontology.Lookup}{noun} \in (Instances) \THEN
   		\BEGIN
		id \GETS \CALL{Ontology.lookup}{noun}\\	
		\RETURN {\mathcal{D} + \{ *\ {\tt sameAs}\ <id> \}}\\
		\END
   \ELSE
    	\mathcal{D} = \mathcal{D} + \{ *\ {\tt type}\ <noun>\} \\
   
   \\
   det \GETS \CALL{GetDeterminant}{group} \\
   \IF det \in {\mbox(possessives)} \THEN
       \mathcal{D} = \mathcal{D} + \{ *\ {\tt isRelatedTo}\ <possessor>\} \\
    
    \IF det \in {\mbox(demonstratives)} \THEN
        \BEGIN
        \IF \CALL{Ontology.Check}{\{<currentSpeaker>\ {\tt focusesOn}\ *\}} \THEN 
            \mathcal{D} = \mathcal{D} + \{<currentSpeaker>\ {\tt focusesOn}\ *\}
        \ELSE
            \mathcal{D} = \mathcal{D} + \CALL{AnaphoricMatching}{} \\
        \END \\
   \\
   adjs \GETS \CALL{GetAdjectives}{group} \\
   \FOREACH adj \in adjs \DO
   	\BEGIN
   		\IF adj == <other> \THEN 
   			\BEGIN
   			id \GETS \CALL{History.GetMatchingGroup}{group} \STMTNUM{8em}{st.history}\\
   			\mathcal{D} = \mathcal{D} + \{ *\ {\tt differentFrom}\ <id> \}\\
   			\RETURN{D}\\
			\END   		
   		\ELSE
	     	\mathcal{D} = \mathcal{D} + \{ *\ {\tt hasFeature}\ <adj>\} \\
    \END\\
    
   \\  
   nounComplements \GETS \CALL{GetNounComplements}{group} \\
   \FOREACH nouncmpl \in nounComplements \DO
     \mathcal{D} = \mathcal{D} + {\CALL{GenerateDescription}{nouncmpl}}\\
   
   
   \\  
   relativeClauses \GETS \CALL{GetSubordinateRelativeClauses}{group} \\
   \FOREACH relative \in relativeClauses \DO
   	\BEGIN
   	 \mathcal{G} \GETS \CALL{GetNominalGroups}{relative} \\
   	 \FOREACH g \in \mathcal{G} \DO
     	\mathcal{D} = \mathcal{D} + {\CALL{GenerateDescription}{g}}
    \END\\
     
   \\
   \RETURN{\mathcal{D}} 
\ENDPROCEDURE
\end{pseudocode}

The method called at (\ref{st.history}) consists in looking through sentences 
that have been stored in the conversation history, then extracting their nominal
group in order to retrieve the identifier of the most recently mentioned 
concept that holds the same characteristics as the nominal group that is being 
processed (algorithm~\ref{algo|History}).

\begin{pseudocode}[ruled]{History.GetMatchingGroup}{group}
\label{algo|History}
\PROCEDURE{History.GetMatchingGroup}{group}
\COMMENT{Extract Nominal group from sentences stored in the history}\\
\mathcal{H} \GETS \CALL{History.GetAllNominalGoup}{}\\
\COMMENT{Generate description of the nominal group that is being processed.} \\
\COMMENT{The adjective  "other" is to be removed before calling this routine} \\
	\mathcal{G} \GETS \CALL{GenerateDescription}{group} \\ 
	
	candidates \GETS \mathcal{H} \cap \mathcal{G}\\
	\IF \left|{candidates}\right| = 0 \THEN
    \BEGIN
      \OUTPUT{\mbox{Couldn't find another object with the same characteristics!}} \\
      \EXIT \\
    \END
   \ELSEIF \left|{candidates}\right| = 1 \THEN
      id \GETS candidates[0]
   \ELSE
   	  id \GETS \CALL{Discrimination}{candidates}\\
   \RETURN{id}
\ENDPROCEDURE
\end{pseudocode}

\end{document}
